# ğŸ“˜ GuÃ­a de la API de Groq - IntegraciÃ³n con OpenAI y Modelo oss120

> **DocumentaciÃ³n basada en https://console.groq.com**  
> GuÃ­a completa para usar la API de Groq con compatibilidad con OpenAI

## ğŸ“‹ Tabla de Contenidos

1. [ğŸ¯ Resumen](#-resumen)
2. [ğŸ”‘ ConfiguraciÃ³n de Credenciales](#-configuraciÃ³n-de-credenciales)
3. [ğŸ“¡ Endpoints de la API](#-endpoints-de-la-api)
4. [ğŸ¤– Modelos Disponibles](#-modelos-disponibles)
5. [ğŸ’» Ejemplos de CÃ³digo](#-ejemplos-de-cÃ³digo)
6. [âš™ï¸ ParÃ¡metros de ConfiguraciÃ³n](#-parÃ¡metros-de-configuraciÃ³n)
7. [ğŸ› SoluciÃ³n de Problemas](#-soluciÃ³n-de-problemas)
8. [ğŸ“Š LÃ­mites y Cuotas](#-lÃ­mites-y-cuotas)

---

## ğŸ¯ Resumen

Groq proporciona una API **compatible con OpenAI**, lo que significa que puedes usar cualquier cliente de OpenAI existente pointing a los endpoints de Groq. Esto facilita la migraciÃ³n y el uso de modelos de alto rendimiento.

**Ventajas de Groq:**
- âš¡ **Inferencia ultrarrÃ¡pida** - Modelos respondiendo en milisegundos
- ğŸ’° **Precios competitivos** - MÃ¡s econÃ³mico que muchos proveedores
- ğŸ”„ **Compatible con OpenAI** - Migra tu cÃ³digo sin cambios
- ğŸ¯ **Modelos especializados** - Optimizados para diferentes casos de uso

---

## ğŸ”‘ ConfiguraciÃ³n de Credenciales

### 1. Obtener tu API Key

1. Ve a [https://console.groq.com](https://console.groq.com)
2. Inicia sesiÃ³n o crea una cuenta
3. Navega a **API Keys** en el menÃº lateral
4. Clic en **Create API Key**
5. Copia tu key y guÃ¡rdala en un lugar seguro

### 2. ConfiguraciÃ³n en tu Proyecto

#### Usando Variables de Entorno

```bash
# En tu archivo .env
GROQ_API_KEY=gsk_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```

#### En Python

```python
import os
from groq import Groq

# OpciÃ³n 1: Usar variable de entorno
client = Groq(
    api_key=os.environ.get("GROQ_API_KEY"),
)

# OpciÃ³n 2: Directamente (no recomendado para producciÃ³n)
client = Groq(
    api_key="gsk_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx",
)
```

---

## ğŸ“¡ Endpoints de la API

### Endpoint Principal

```
https://api.groq.com/openai/v1
```

### Endpoints Disponibles

| MÃ©todo | Endpoint | DescripciÃ³n |
|--------|----------|-------------|
| `POST` | `/chat/completions` | Generar respuestas de chat |
| `POST` | `/completions` | GeneraciÃ³n de texto |
| `POST` | `/embeddings` | Obtener embeddings |
| `GET` | `/models` | Listar modelos disponibles |
| `GET` | `/models/{model_id}` | Obtener detalles de un modelo |

### Estructura de Chat Completion

```json
POST https://api.groq.com/openai/v1/chat/completions

{
  "model": "llama-3.1-8b-instant",
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful assistant."
    },
    {
      "role": "user",
      "content": "Hello!"
    }
  ],
  "temperature": 0.7,
  "max_tokens": 1000,
  "stream": false
}
```

---

## ğŸ¤– Modelos Disponibles

### Modelos de Chat (Chat Completions)

| Modelo | DescripciÃ³n | Contexto | Velocidad |
|--------|-------------|----------|-----------|
| `llama-3.3-70b-versatile` | Llama 3.3 70B - VersÃ¡til | 128K | RÃ¡pido |
| `llama-3.1-8b-instant` | Llama 3.1 8B - InstantÃ¡neo | 128K | Muy RÃ¡pido |
| `llama-3.2-1b-preview` | Llama 3.2 1B - Preview | 128K | UltrarrÃ¡pido |
| `llama-3.2-3b-preview` | Llama 3.2 3B - Preview | 128K | RÃ¡pido |
| `gemma-7b-it` | Gemma 7B - Instruction | 8K | RÃ¡pido |
| **`gpt-oss-120b`** | OpenAI OSS 120B - Premium | 128K | RÃ¡pido â­ |

### CaracterÃ­sticas del Modelo OSS120

El modelo **`gpt-oss-120b`** es un modelo premium de OpenAI disponible a travÃ©s de Groq con las siguientes caracterÃ­sticas:

- ğŸ“ **128K tokens de contexto** - Capacidad para conversaciones largas
- ğŸ§  **120B parÃ¡metros** - Alta capacidad de razonamiento
- ğŸ¯ **Optimizado para asistencia** - Ideal para chatbots y soporte
- âš¡ **Alta velocidad de inferencia** - Respuestas rÃ¡pidas
- ğŸ’° **Precio competitivo** - MÃ¡s econÃ³mico que la API directa de OpenAI

---

## ğŸ’» Ejemplos de CÃ³digo

### Python con SDK de Groq

```python
from groq import Groq
import os

# Inicializar cliente
client = Groq(api_key=os.environ.get("GROQ_API_KEY"))

# Chat bÃ¡sico
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "system",
            "content": "Eres un asistente amable y Ãºtil."
        },
        {
            "role": "user",
            "content": "Â¿CÃ³mo puedo configurar mi VPN?"
        }
    ],
    model="gpt-oss-120b",  # Usando el modelo OSS120
    temperature=0.7,
    max_tokens=1000,
)

print(chat_completion.choices[0].message.content)
```

### Python con Cliente OpenAI (Compatible)

```python
from openai import OpenAI

# Groq es compatible con OpenAI - solo cambia el base_url
client = OpenAI(
    api_key="gsk_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx",
    base_url="https://api.groq.com/openai/v1"
)

# El cÃ³digo es idÃ©ntico a OpenAI
response = client.chat.completions.create(
    model="gpt-oss-120b",
    messages=[
        {"role": "user", "content": "Explica quÃ© es WireGuard"}
    ],
    temperature=0.5,
)

print(response.choices[0].message.content)
```

### Streaming Responses

```python
from groq import Groq

client = Groq(api_key="tu_api_key")

stream = client.chat.completions.create(
    messages=[
        {"role": "user", "content": "Crea un tutorial de 5 pasos para usar VPN"}
    ],
    model="gpt-oss-120b",
    stream=True,
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

### IntegraciÃ³n con uSipipo

```python
# infrastructure/api_clients/groq_client.py

from groq import Groq
from config import settings
import logging

logger = logging.getLogger(__name__)

class GroqClient:
    """Cliente para la API de Groq con soporte para modelos OSS120."""
    
    def __init__(self):
        self.client = Groq(api_key=settings.GROQ_API_KEY)
        self.default_model = settings.GROQ_MODEL  # gpt-oss-120b
    
    async def get_chat_response(
        self,
        messages: list,
        model: str = None,
        temperature: float = 0.7,
        max_tokens: int = 1000
    ) -> str:
        """
        Obtener respuesta del modelo de chat.
        
        Args:
            messages: Lista de mensajes [{role, content}]
            model: Modelo a usar (default: gpt-oss-120b)
            temperature: Creatividad (0.0-2.0)
            max_tokens: MÃ¡ximo de tokens en respuesta
            
        Returns:
            Respuesta del modelo como string
        """
        try:
            chat_completion = self.client.chat.completions.create(
                messages=messages,
                model=model or self.default_model,
                temperature=temperature,
                max_tokens=max_tokens,
            )
            return chat_completion.choices[0].message.content
            
        except Exception as e:
            logger.error(f"Error en Groq API: {e}")
            raise
    
    async def stream_response(
        self,
        messages: list,
        model: str = None
    ):
        """Generar respuesta con streaming."""
        stream = self.client.chat.completions.create(
            messages=messages,
            model=model or self.default_model,
            stream=True,
        )
        
        for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

# Uso en el servicio de IA
groq_client = GroqClient()

async def get_sip_response(user_message: str, context: list) -> str:
    """Obtener respuesta de Sip AI."""
    messages = [
        {"role": "system", "content": "Eres Sip, asistente de VPN de uSipipo."},
        *context,
        {"role": "user", "content": user_message}
    ]
    
    return await groq_client.get_chat_response(
        messages=messages,
        model="gpt-oss-120b",
        temperature=0.7,
        max_tokens=1000
    )
```

### Node.js / JavaScript

```javascript
import { Groq } from "groq-sdk";

const groq = new Groq({
  apiKey: process.env.GROQ_API_KEY,
});

async function chat() {
  const chatCompletion = await groq.chat.completions.create({
    messages: [
      { role: "system", content: "You are a helpful VPN assistant." },
      { role: "user", content: "How do I configure WireGuard?" }
    ],
    model: "gpt-oss-120b",
    temperature: 0.7,
  });

  console.log(chatCompletion.choices[0].message.content);
}

chat();
```

### cURL

```bash
curl -X POST "https://api.groq.com/openai/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer gsk_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx" \
  -d '{
    "model": "gpt-oss-120b",
    "messages": [
      {
        "role": "system",
        "content": "You are Sip, a helpful VPN assistant."
      },
      {
        "role": "user",
        "content": "What is the difference between WireGuard and Outline?"
      }
    ],
    "temperature": 0.7,
    "max_tokens": 1000
  }'
```

---

## âš™ï¸ ParÃ¡metros de ConfiguraciÃ³n

### ParÃ¡metros de Chat Completion

| ParÃ¡metro | Tipo | Default | Rango | DescripciÃ³n |
|-----------|------|---------|-------|-------------|
| `model` | string | Requerido | - | Modelo a usar |
| `messages` | array | Requerido | - | Mensajes de la conversaciÃ³n |
| `temperature` | float | 0.7 | 0.0-2.0 | Creatividad de respuestas |
| `max_tokens` | int | 1000 | 1-4096 | MÃ¡ximo de tokens en respuesta |
| `top_p` | float | 1.0 | 0.0-1.0 | Nucleus sampling |
| `stream` | bool | false | - | Respuesta en streaming |
| `stop` | array | null | - | Secuencias de parada |
| `presence_penalty` | float | 0.0 | -2.0 a 2.0 | PenalizaciÃ³n por presencia |
| `frequency_penalty` | float | 0.0 | -2.0 a 2.0 | PenalizaciÃ³n por frecuencia |

### ConfiguraciÃ³n Recomendada para Sip AI

```python
# ConfiguraciÃ³n optimizada para asistente de VPN
config = {
    "model": "gpt-oss-120b",
    "temperature": 0.7,        # Balance entre creatividad y consistencia
    "max_tokens": 1000,        # Respuestas completas pero no excesivas
    "top_p": 0.95,             # Buena diversidad de respuestas
    "presence_penalty": 0.0,   # Mantenerè¯é¢˜ relevante
    "frequency_penalty": 0.0,  # Evitar repeticiones
}
```

---

## ğŸ› SoluciÃ³n de Problemas

### Error: "Invalid API Key"

```bash
# Verificar que la API key sea correcta
echo $GROQ_API_KEY

# Debe empezar con "gsk_"
```

**SoluciÃ³n:**
1. Verifica que la API key estÃ© correctamente configurada
2. AsegÃºrate de no tener espacios extra
3. Regenera la key si es necesario

### Error: "Model not found"

```python
# Verificar modelos disponibles
import requests

response = requests.get(
    "https://api.groq.com/openai/v1/models",
    headers={"Authorization": f"Bearer {api_key}"}
)
print(response.json())
```

**SoluciÃ³n:**
1. Verifica que el nombre del modelo sea correcto
2. Algunos modelos pueden estar en preview

### Error: "Rate limit exceeded"

```python
import time
from groq import Groq

client = Groq(api_key="tu_key")

# Implementar retry con backoff
for attempt in range(3):
    try:
        response = client.chat.completions.create(...)
        break
    except Exception as e:
        if "rate_limit" in str(e):
            time.sleep(2 ** attempt)  # Exponential backoff
        else:
            raise
```

**SoluciÃ³n:**
1. Implementar rate limiting en tu aplicaciÃ³n
2. Usar cacheo de respuestas cuando sea posible
3. Contactar a soporte si los lÃ­mites son muy restrictivos

### Error: "Connection timeout"

```python
import httpx
from groq import Groq

# Usar cliente con timeout configurado
client = Groq(
    api_key="tu_key",
    http_client=httpx.Client(
        timeout=30.0,  # 30 segundos
        verify=True
    )
)
```

---

## ğŸ“Š LÃ­mites y Cuotas

### LÃ­mites de Rate (TÃ­picos)

| Plan | RPM (Requests/Min) | TPM (Tokens/Min) |
|------|-------------------|------------------|
| Free | 10 | 10,000 |
| Pro | 100 | 500,000 |

### Precios (Ejemplos)

| Modelo | Input ($/1M tokens) | Output ($/1M tokens) |
|--------|---------------------|----------------------|
| llama-3.1-8b-instant | $0.04 | $0.04 |
| llama-3.3-70b-versatile | $0.59 | $0.79 |
| **gpt-oss-120b** | $1.00 | $1.50 |

### OptimizaciÃ³n de Costos

```python
# Estrategias para reducir costos

# 1. Limitar max_tokens
async def get_response(user_message: str) -> str:
    response = await client.chat.completions.create(
        model="llama-3.1-8b-instant",  # MÃ¡s econÃ³mico
        messages=[{"role": "user", "content": user_message}],
        max_tokens=500,  # Limitar respuesta
    )
    return response.choices[0].message.content

# 2. Usar caching
from functools import lru_cache

@lru_cache(maxsize=100)
def cached_response(question: str) -> str:
    # Preguntas frecuentes cacheadas
    return get_response_sync(question)

# 3. Resumir contexto
async def summarize_context(context: str) -> str:
    """Resume el contexto para reducir tokens."""
    response = await client.chat.completions.create(
        model="llama-3.1-8b-instant",
        messages=[
            {"role": "system", "content": "Resume esto en 100 palabras:"},
            {"role": "user", "content": context}
        ],
        max_tokens=100,
    )
    return response.choices[0].message.content
```

---

## ğŸ” Mejores PrÃ¡cticas de Seguridad

### 1. No expongas tu API Key

```python
# âŒ MALO - Key en cÃ³digo
api_key = "gsk_xxxxxxxxxxxxxxxx"

# âœ… BUENO - Usar variables de entorno
import os
api_key = os.environ.get("GROQ_API_KEY")
```

### 2. Usa .env para desarrollo

```bash
# .env
GROQ_API_KEY=gsk_xxxxxxxxxxxxxxxx
```

```python
# .gitignore
.env
*.env
```

### 3. Implementa logging seguro

```python
import logging

# Configurar logging sin exponer datos sensibles
logging.basicConfig(level=logging.INFO)

def log_interaction(user_id: str, response_length: int):
    logging.info(f"User {user_id} - Response length: {response_length} chars")
    # NO loguear el contenido de los mensajes
```

### 4. Valida inputs de usuario

```python
import re

def sanitize_message(message: str) -> str:
    """Sanitizar mensaje para prevenir inyecciones."""
    # Eliminar caracteres especiales problemÃ¡ticos
    cleaned = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', message)
    # Limitar longitud
    return cleaned[:4000]  # Max contexto
```

---

## ğŸ“ˆ IntegraciÃ³n con uSipipo

### ConfiguraciÃ³n en config.py

```python
# config.py

from pydantic_settings import BaseSettings
from typing import Optional
from functools import lru_cache

class Settings(BaseSettings):
    # ... otras configuraciones ...
    
    # Groq AI Configuration
    GROQ_API_KEY: Optional[str] = None
    GROQ_MODEL: str = "gpt-oss-120b"  # Modelo por defecto
    GROQ_TEMPERATURE: float = 0.7
    GROQ_MAX_TOKENS: int = 1000
    GROQ_TIMEOUT: int = 15
    
    @property
    def is_groq_configured(self) -> bool:
        return bool(self.GROQ_API_KEY)
    
    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
        case_sensitive = True

@lru_cache()
def get_settings():
    return Settings()

settings = get_settings()
```

### Uso en AI Support Service

```python
# application/services/ai_support_service.py

from infrastructure.api_clients.groq_client import GroqClient
from config import settings

class AISupportService:
    """Servicio de soporte con IA."""
    
    def __init__(self):
        self.groq_client = GroqClient() if settings.is_groq_configured else None
    
    async def get_assistant_response(
        self,
        user_message: str,
        conversation_history: list = None
    ) -> str:
        """Obtener respuesta del asistente de IA."""
        
        if not self.groq_client:
            return "âš ï¸ El servicio de IA no estÃ¡ configurado."
        
        # Construir mensajes
        system_prompt = """Eres Sip, el asistente de VPN de uSipipo.
        Ayudas a usuarios con problemas de conexiÃ³n VPN, configuraciÃ³n
        y seguridad. SÃ© amable, claro y conciso."""
        
        messages = [{"role": "system", "content": system_prompt}]
        
        if conversation_history:
            messages.extend(conversation_history)
        
        messages.append({"role": "user", "content": user_message})
        
        try:
            response = await self.groq_client.get_chat_response(
                messages=messages,
                model=settings.GROQ_MODEL,
                temperature=settings.GROQ_TEMPERATURE,
                max_tokens=settings.GROQ_MAX_TOKENS
            )
            return response
            
        except Exception as e:
            logging.error(f"Error getting AI response: {e}")
            return "âš ï¸ Lo siento, tuve un problema procesando tu solicitud."
```

---

## ğŸ”— Recursos Adicionales

- ğŸ“– [DocumentaciÃ³n Oficial de Groq](https://console.groq.com/docs)
- ğŸ’» [SDK Python de Groq](https://github.com/groq/groq-sdk)
- ğŸ“š [Referencia de la API OpenAI](https://platform.openai.com/docs/api-reference)
- ğŸ› [Reportar Issues](https://github.com/groq/groq-sdk/issues)

---

## ğŸ“ Notas de la VersiÃ³n

| VersiÃ³n | Fecha | Cambios |
|---------|-------|---------|
| 1.0.0 | 2026-01-09 | DocumentaciÃ³n inicial para uSipipo |
| 1.0.1 | 2026-01-09 | AÃ±adido modelo gpt-oss-120b |

---

<div align="center">

**ğŸ“˜ DocumentaciÃ³n de la API de Groq para uSipipo**  
*IntegraciÃ³n rÃ¡pida y sencilla con modelos de alto rendimiento*

[â¬†ï¸ Volver al inicio](#-guÃ­a-de-la-api-de-groq---integraciÃ³n-con-openai-y-modelo-oss120)

</div>
