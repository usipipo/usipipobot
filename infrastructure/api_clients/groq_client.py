"""
Cliente de infraestructura para la API de Groq - Asistente IA Sip.

Basado en la documentaciÃ³n oficial de Groq:
- https://console.groq.com/docs/quickstart
- https://console.groq.com/docs/text-chat
- https://console.groq.com/docs/model/openai/gpt-oss-120b

Author: uSipipo Team
Version: 2.0.0
"""
from typing import List, Dict, Optional
from groq import Groq, AsyncGroq
from groq import RateLimitError, APIConnectionError, APIStatusError
from config import settings
from utils.logger import logger

class GroqClient:
    """Cliente de infraestructura para API de Groq."""
    
    # Actualizamos el diccionario con los datos REALES que me diste
    MODELS = {
        "openai/gpt-oss-120b": {
            "description": "GPT OSS 120B - High capability agentic model",
            "context_window": 131072,
        }
    }
    
    def __init__(self):
        """Inicializa el cliente de Groq con configuraciÃ³n."""
        
        # 1. VerificaciÃ³n explÃ­cita de la API Key
        if not settings.GROQ_API_KEY:
            logger.critical("âŒ ERROR CRÃTICO: GROQ_API_KEY estÃ¡ vacÃ­a en config.py o .env")
            # Esto ayudarÃ¡ a ver si el problema es la llave
        
        try:
            # Cliente sÃ­ncrono
            self.client = Groq(api_key=settings.GROQ_API_KEY)
            # Cliente asÃ­ncrono
            self.async_client = AsyncGroq(api_key=settings.GROQ_API_KEY)
        except Exception as e:
            logger.error(f"âŒ Error al crear cliente Groq: {e}")
        
        # 2. IMPORTANTE: Usar el modelo de settings, no el hardcodeado
        # Si settings.GROQ_MODEL tiene valor, lo usa. Si no, usa el string directo.
        self.model = settings.GROQ_MODEL if settings.GROQ_MODEL else "openai/gpt-oss-120b"
        
        self.temperature = settings.GROQ_TEMPERATURE
        self.max_tokens = settings.GROQ_MAX_TOKENS
        self.timeout = settings.GROQ_TIMEOUT
        
        logger.info(f"ðŸŒŠ GroqClient inicializado. Modelo objetivo: {self.model}")

    async def chat_completion(self, messages: List[Dict[str, str]]) -> str:
        """
        Genera una respuesta de chat asÃ­ncrona usando el modelo configurado.
        """
        try:
            # Log para depuraciÃ³n: Ver quÃ© estamos enviando
            logger.debug(f"ðŸ“¤ Enviando request a Groq: {self.model}")
            
            completion = await self.async_client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                timeout=self.timeout
            )
            
            # Extraer respuesta
            response_content = completion.choices[0].message.content
            
            # Log de uso de tokens (Ãºtil para ver si conecta)
            if hasattr(completion, 'usage'):
                logger.info(f"ðŸ’° Tokens usados: {completion.usage.total_tokens}")
                
            return response_content

        except APIConnectionError:
            logger.error("âŒ Error de conexiÃ³n con Groq. Revisa tu internet o DNS.")
            return "Lo siento, tengo problemas de conexiÃ³n en este momento."
            
        except RateLimitError:
            logger.error("âš ï¸ LÃ­mite de velocidad alcanzado (Rate Limit).")
            return "Estoy un poco saturado, por favor intenta en unos segundos."
            
        except APIStatusError as e:
            logger.error(f"âŒ Error de estado API ({e.status_code}): {e.message}")
            if e.status_code == 404:
                 logger.error(f"ðŸ” El modelo '{self.model}' no fue encontrado. Verifica el nombre exacto.")
            return "OcurriÃ³ un error en el servidor de IA."
            
        except Exception as e:
            logger.error(f"âŒ Error inesperado en Groq: {str(e)}")
            return "OcurriÃ³ un error inesperado al procesar tu mensaje."
        
# from typing import List, Dict
# from groq import Groq, AsyncGroq
# from groq import RateLimitError, APIConnectionError, APIStatusError
# from config import settings
# from utils.logger import logger


# class GroqClient:
#     """Cliente de infraestructura para API de Groq."""
    
#     # Modelo por defecto utilizado
#     DEFAULT_MODEL = "openai/gpt-oss-120b"
    
#     MODELS = {
#         DEFAULT_MODEL: {
#             "description": "Modelo GPT-OSS 120B de OpenAI (disponible en Groq)",
#             "context_window": 131072,
#         },
#     }
    
#     def __init__(self):
#         """Inicializa el cliente de Groq con configuraciÃ³n."""
#         if not settings.GROQ_API_KEY:
#             logger.warning("âš ï¸ GROQ_API_KEY no configurada. Sip no funcionarÃ¡ correctamente.")
        
#         # Cliente sÃ­ncrono (para operaciones que no requieren async)
#         self.client = Groq(api_key=settings.GROQ_API_KEY)
#         # Cliente asÃ­ncrono (para operaciones async)
#         self.async_client = AsyncGroq(api_key=settings.GROQ_API_KEY)
        
#         # ConfiguraciÃ³n del modelo (hardcodeado para usar solo GPT-OSS 120B)
#         self.model = self.DEFAULT_MODEL
#         self.temperature = settings.GROQ_TEMPERATURE
#         self.max_tokens = settings.GROQ_MAX_TOKENS
#         self.timeout = settings.GROQ_TIMEOUT
        
#         logger.info(f"ðŸŒŠ GroqClient inicializado con modelo: {self.model}")
    
#     async def chat_completion(self, messages: List[Dict[str, str]]) -> str:
#         """
#         Realiza peticiÃ³n de chat completion a Groq de forma asÃ­ncrona.
        
#         Args:
#             messages: Lista de mensajes en formato dict [{"role": "user", "content": "..."}]
#                       Roles soportados: "system", "user", "assistant"
            
#         Returns:
#             str: Respuesta generada por el modelo
            
#         Raises:
#             ValueError: Si la API key no estÃ¡ configurada
#             RateLimitError: Si se excede el lÃ­mite de llamadas
#             APIConnectionError: Si hay error de conexiÃ³n
#             APIStatusError: Si hay error en la respuesta de la API
#             Exception: Para otros errores
#         """
#         if not self.validate_api_key():
#             raise ValueError("API key de Groq no configurada o invÃ¡lida")
        
#         try:
#             logger.debug(f"ðŸŒŠ Enviando {len(messages)} mensajes a Groq API")
#             logger.debug(f"ðŸŒŠ Modelo: {self.model}, Timeout: {self.timeout}s")
            
#             # Usar cliente asÃ­ncrono para mantener el event loop libre
#             response = await self.async_client.chat.completions.create(
#                 messages=messages,
#                 model=self.model,
#                 temperature=self.temperature,
#                 max_tokens=self.max_tokens,
#                 timeout=float(self.timeout) if self.timeout else None
#             )
            
#             logger.debug(f"ðŸŒŠ Respuesta recibida de Groq: {response}")
            
#             if response.choices and len(response.choices) > 0:
#                 content = response.choices[0].message.content
#                 if content:
#                     logger.debug(f"ðŸŒŠ Contenido de respuesta: {len(content)} caracteres")
#                     return content
#                 else:
#                     logger.error("ðŸŒŠ Groq API devolviÃ³ contenido vacÃ­o en la respuesta")
#                     raise ValueError("La API de Groq devolviÃ³ una respuesta vacÃ­a")
#             else:
#                 logger.error(f"ðŸŒŠ Groq API no devolviÃ³ choices. Response: {response}")
#                 raise ValueError("La API de Groq no devolviÃ³ ninguna opciÃ³n de respuesta")
                
#         except RateLimitError as e:
#             logger.error(f"ðŸŒŠ Rate limit excedido en Groq API: {str(e)}")
#             raise ValueError("Has excedido el lÃ­mite de llamadas a la IA. Por favor, espera un momento.") from e
        
#         except APIConnectionError as e:
#             logger.error(f"ðŸŒŠ Error de conexiÃ³n con Groq API: {str(e)}")
#             raise ValueError("No se pudo conectar con el servicio de IA. Verifica tu conexiÃ³n a internet.") from e
        
#         except APIStatusError as e:
#             logger.error(f"ðŸŒŠ Error de estado en Groq API: {str(e)}")
#             raise ValueError(f"Error del servicio de IA: cÃ³digo {e.status_code}") from e
        
#         except (ValueError, TypeError, KeyError, AttributeError, TimeoutError) as e:
#             error_type = type(e).__name__
#             error_msg = str(e)
#             logger.error(f"ðŸŒŠ Error en Groq API [{error_type}]: {error_msg}")
            
#             if "timeout" in error_msg.lower():
#                 raise ValueError("Sip estÃ¡ tardando mucho en responder. Por favor, intenta con un mensaje mÃ¡s corto.") from e
#             elif "authentication" in error_msg.lower() or "api key" in error_msg.lower():
#                 raise ValueError("Error de autenticaciÃ³n con Sip. Contacta al administrador.") from e
#             elif "rate limit" in error_msg.lower():
#                 raise ValueError("Sip estÃ¡ recibiendo muchas solicitudes. Por favor, espera un momento.") from e
#             elif "model" in error_msg.lower():
#                 raise ValueError("El modelo de IA no estÃ¡ disponible. Contacta al administrador.") from e
#             else:
#                 raise ValueError(f"Error al comunicarse con Sip: {error_msg}") from e
    
    def chat_completion_sync(self, messages: List[Dict[str, str]]) -> str:
        """
        Realiza peticiÃ³n de chat completion a Groq de forma sÃ­ncrona.
        
        Args:
            messages: Lista de mensajes en formato dict
            
        Returns:
            str: Respuesta generada por el modelo
        """
        if not self.validate_api_key():
            raise ValueError("API key de Groq no configurada o invÃ¡lida")
        
        try:
            logger.debug(f"ðŸŒŠ [SYNC] Enviando {len(messages)} mensajes a Groq API")
            logger.debug(f"ðŸŒŠ [SYNC] Modelo: {self.model}")
            
            response = self.client.chat.completions.create(
                messages=messages,
                model=self.model,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                timeout=float(self.timeout) if self.timeout else None
            )
            
            if response.choices and len(response.choices) > 0:
                content = response.choices[0].message.content
                if content:
                    return content
                else:
                    raise ValueError("La API de Groq devolviÃ³ una respuesta vacÃ­a")
            else:
                raise ValueError("La API de Groq no devolviÃ³ ninguna opciÃ³n de respuesta")
                
        except (RateLimitError, APIConnectionError, APIStatusError) as e:
            logger.error(f"ðŸŒŠ [SYNC] Error en Groq API: {str(e)}")
            raise ValueError(f"Error al comunicarse con Sip: {str(e)}") from e
    
    def stream_chat_completion(self, messages: List[Dict[str, str]]):
        """
        Realiza streaming de chat completion desde Groq.
        
        Args:
            messages: Lista de mensajes en formato dict
            
        Yields:
            str: Fragmentos de la respuesta generada
        """
        if not self.validate_api_key():
            raise ValueError("API key de Groq no configurada o invÃ¡lida")
        
        try:
            logger.debug(f"ðŸŒŠ [STREAM] Iniciando streaming con modelo: {self.model}")
            
            stream = self.client.chat.completions.create(
                messages=messages,
                model=self.model,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                stream=True
            )
            
            for chunk in stream:
                if chunk.choices and len(chunk.choices) > 0:
                    content = chunk.choices[0].delta.content
                    if content:
                        yield content
                        
        except (RateLimitError, APIConnectionError, APIStatusError) as e:
            logger.error(f"ðŸŒŠ [STREAM] Error en streaming: {str(e)}")
            raise ValueError(f"Error en streaming: {str(e)}") from e
    
    def validate_api_key(self) -> bool:
        """
        Valida si la API key estÃ¡ configurada correctamente.
        
        Returns:
            bool: True si la API key es vÃ¡lida
        """
        return bool(settings.GROQ_API_KEY and len(settings.GROQ_API_KEY) > 10)
    
    def get_model_info(self) -> Dict[str, str]:
        """
        Retorna informaciÃ³n sobre la configuraciÃ³n del modelo.
        
        Returns:
            Dict: InformaciÃ³n del modelo
        """
        model_info = self.MODELS.get(self.model, {})
        return {
            "model": self.model,
            "description": model_info.get("description", "Modelo personalizado"),
            "context_window": str(model_info.get("context_window", "N/A")),
            "temperature": str(self.temperature),
            "max_tokens": str(self.max_tokens),
            "timeout": str(self.timeout),
            "api_key_configured": self.validate_api_key()
        }
    
    def get_available_models(self) -> Dict[str, Dict]:
        """
        Retorna el modelo disponible en Groq.
        
        Returns:
            Dict: InformaciÃ³n del modelo disponible
        """
        return self.MODELS
    
    async def test_connection(self) -> bool:
        """
        Prueba la conexiÃ³n con la API de Groq.
        
        Returns:
            bool: True si la conexiÃ³n es exitosa
        """
        try:
            logger.info("ðŸŒŠ Probando conexiÃ³n con Groq API...")
            
            test_messages = [
                {"role": "system", "content": "Eres un asistente de prueba."},
                {"role": "user", "content": "Responde con 'OK' si puedes leer esto."}
            ]
            
            response = await self.chat_completion(test_messages)
            success = "OK" in response.upper()
            
            if success:
                logger.info("âœ… ConexiÃ³n con Groq API exitosa")
            else:
                logger.warning(f"âš ï¸ Respuesta inesperada en test: {response}")
            
            return success
            
        except (RateLimitError, APIConnectionError, APIStatusError) as e:
            logger.error(f"ðŸŒŠ Error en test de conexiÃ³n: {str(e)}")
            return False
    
    def set_model(self, model_name: str) -> bool:
        """
        Cambia el modelo utilizado por el cliente.
        Nota: Por defecto solo estÃ¡ disponible openai/gpt-oss-120b.
        
        Args:
            model_name: Nombre del modelo a usar
            
        Returns:
            bool: True si el modelo fue cambiado exitosamente
        """
        if model_name in self.MODELS:
            self.model = model_name
            logger.info(f"ðŸŒŠ Modelo cambiado a: {model_name}")
            return True
        else:
            logger.error(f"ðŸŒŠ Modelo no encontrado: {model_name}")
            logger.info(f"ðŸŒŠ Solo disponible: {self.DEFAULT_MODEL}")
            return False
